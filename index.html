<!DOCTYPE HTML>
<html lang="en"><head>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6J8CBRENW5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6J8CBRENW5');
</script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhijian Huang</title>
  
  <meta name="author" content="Zhijian Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!--link rel="icon" type="image/png" href="images/seal_icon.png"-->

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhijian Huang</name>
              </p>
              <p style="text-align:center">I am currently a M.Sc student at Sun Yat-sen University.
              </p>

              <p style="text-align:center">
                <a href="https://zhijian11.github.io/">Home</a> &nbsp/&nbsp
                <a href="mailto:zhijianh11@gmail.com">Email</a> &nbsp/&nbsp
                <!--<a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp-->
                <a href="https://github.com/zhijian11/">Github</a> 
                <!-- &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?hl=zh-CN&user=jc3YlxEAAAAJ">Google Scholar</a> -->
              </p>
            </td>
            <td style="padding:8%;width:40%;max-width:40%">
              <!-- <img style="width:80%;max-width:100%" alt="profile photo" src="resources/pp.jpeg" class="hoverZoomLink"> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests lie in the areas of <em>computer vision</em> and <em>deep learning</em>. I am particularly interested in
                <!-- <em>object detection</em>, including <em>2D</em> object detection, <em>3D</em> object detection (both vision-based and LiDAR-based) and <em>open-vocabulary</em> object detection. -->
                <em>autonomous driving</em> and <em>large language model</em>.
              </p>
            </td>s
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/RoboTron-Drive.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="RoboTron-Drive/index.html">
                <papertitle>RoboTron-Drive: All-in-One Large Multimodal Model for Autonomous Driving</papertitle>
              </a>
              <br>
              <strong>Zhijian Huang</strong>, 
              <a href="https://fcjian.github.io/">Chengjian Feng</a>, 
              <a href="https://scholar.google.com.hk/citations?user=gO4divAAAAAJ&hl=zh-CN&oi=sra">Fen Yan</a>, 
              <a href="xbh23@mails.tsinghua.edu.cn">Baihui Xiao</a>, 
              <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">Zequn Jie</a>, 
              <a href="https://y-zhong.info/">Yujie Zhong</a>,
              <a href="https://lemondan.github.io/">Xiaodan Liang</a>, 
              <a href="http://forestlinma.com/">Lin Ma</a>
              <!-- <br>
              <em>ECCV</em>, 2024
              <font color="red"><strong>(Oral)</strong></font> -->
              <br>
              <a href="RoboTron-Drive/index.html">project page</a> /
              <a href="">arXiv</a>
              <p></p>We propose a novel all-in-one large multimodal model, RoboTron-Drive, robustly equipped with the general capabilities and the generalization ability.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/RDA_Drive.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="RDA_Drive/index.html">
                <papertitle>Making Large Language Models Better Planners with Reasoning-Decision Alignment</papertitle>
              </a>
              <br>
              <strong>Zhijian Huang</strong>, 
              <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=1ltylFwAAAAJ&view_op=list_works&sortby=pubdate">Tao Tang</a>, 
              <a href="https://scholar.google.co.jp/citations?user=WL5mbfEAAAAJ&hl=zh-CN&oi=ao">Shaoxiang Chen</a>, 
              <a href="https://github.com/sihaoevery">Sihao Lin</a>, 
              <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">Zequn Jie</a>, 
              <a href="http://forestlinma.com/">Lin Ma</a>,
              <a href="https://wanggrun.github.io/">Guangrun Wang</a>, 
              <a href="https://lemondan.github.io/">Xiaodan Liang</a>
              <br>
              <em>ECCV</em>, 2024
              <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="RDA_Drive/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2408.13890">arXiv</a>
              <p></p>
              <p>We introduce a multimodal large language decision-making model RDA-Driver with the reasoning-decision alignment. </p>
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/instagen.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="InstaGen/index.html">
                <papertitle>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">
							Zequn Jie</a>, <a href="https://weidixie.github.io/">
							Weidi Xie</a>, <a href="http://forestlinma.com/">
							Lin Ma</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="InstaGen/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2402.05937">arXiv</a>
              <p></p>
              <p>We introduce a novel paradigm to enhance the ability of object detector by training on synthetic dataset generated from diffusion models.</p>
            </td>
          </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:top">
            <img src="resources/aedet.png" alt="clean-usnob" width="180">
          </td>
          <td width="75%" valign="middle">
            <a href="aedet/index.html">
              <papertitle>AeDet: Azimuth-invariant Multi-view 3D Object Detection</papertitle>
            </a>
            <br>
              <strong>Chenjian Feng</strong>,
              <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">Zequn Jie</a>,
              <a href="https://y-zhong.info/">Yujie Zhong</a>,
              <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN&oi=ao">Xiangxiang Chu</a>,
              <a href="http://forestlinma.com/">Lin Ma</a>
            <br>
            <em>CVPR</em>, 2023
            <br>
            <a href="aedet/index.html">project page</a> /
            <a href="https://arxiv.org/abs/2211.12501">arXiv</a>
            <p></p>
            <p>We propose an Azimuth-equivariant Detector (AeDet) that is able to perform azimuth-invariant multi-view 3D object detection.</p>
            <br>
          </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/promptdet.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="promptdet/index.html">
                <papertitle>PromptDet: Towards Open-vocabulary Detection using Uncurated Images</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN&oi=ao">
							Zequn Jie</a>, <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN&oi=ao">
							Xiangxiang Chu</a>, <a href="https://scholar.google.com/citations?user=qq6hueYAAAAJ&hl=zh-CN&oi=ao">
							Haibing Ren</a>, <a href="https://scholar.google.com/citations?user=s5b7lU4AAAAJ&hl=zh-CN&oi=ao">
							Xiaolin Wei</a>, <a href="https://weidixie.github.io/">
							Weidi Xie</a>, <a href="http://forestlinma.com/">
							Lin Ma</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="promptdet/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2203.16513">arXiv</a>
              <p></p>
              <p>We propose an open-vocabulary object detector PromptDet, which is able to detect novel categories without any manual annotations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/tood.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="tood/index.html">
                <papertitle>TOOD: Task-aligned One-stage Object Detection</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="">Yu Gao</a>, <a href="">Matthew R. Scott</a>, <a href="http://whuang.org/">Weilin Huang</a>
              <br>
              <em>ICCV</em>, 2021
              <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="tood/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2108.07755">arXiv</a>
              <p></p>
              <p>We propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the classification and localization tasks in a learning-based manner.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/loce.png" alt="clean-usnob" width="180">
            </td>
            <td width="75%" valign="middle">
              <a href="loce/index.html">
                <papertitle>Exploring Classification Equilibrium in Long-Tailed Object Detection</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="http://whuang.org/">Weilin Huang</a>
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="loce/index.html">project page</a> /
              <a href="https://arxiv.org/abs/2108.07507">arXiv</a>
              <p></p>
              <p>We balance the classification of the long-tailed detector via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resources/sbada_gan.png" alt="clean-usnob" width="180" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219304941">
                <papertitle>Domain adaptation with SBADA-GAN and Mean Teacher</papertitle>
              </a>
              <br>
              <strong>Chenjian Feng</strong>, <a href="">Zhaoshui He</a>, <a href="">Jiawei Wang</a>, <a href="">Qinzhuang Lin</a>, <a href="">Zhouping Zhu</a>, <a href="">Jun Lv</a>, <a href="">Shengli Xie</a>
              <br>
              <em>Neurocomputing</em>, 2020
              <br>
              <p></p>
              <p>We propose a powerful model for unsupervised domain adaptation by introducing  Mean Teacher as a target classifier of SBADA-GAN.</p>
            </td>
          </tr> -->

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template gratefully stolen from <a href="https://github.com/jonbarron/website/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
